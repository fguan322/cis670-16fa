# Sept 8, 2016

+ Keep signing up for lecture topics!

+ Keep thinking off semester projects. Don't forget that you can work in groups!

+ Keep reading Ch4.v and start looking at the exercises in Ch5.v and Ch6.v.
  Also read PFPL chapters 4-6.

+ Don't forget to send questions before EACH class.


# Questions

[From before + new ones]

## Questions about co-finite quantification & induction modulo renaming

+ 1) why we could define both cofinite quantification and exists-fresh version of typing rule for let?
  2) What does admissible mean for let ?

+ I am not clear about the meaning of `L` in the definition of `lc_let`. Is
  there any specific meaning of `L`? What will be the problem if `L` is empty?

+ I'm kind of confused by the induction principle stated in PFPL. It seems
  weird that P is parametrized by X because it suggests that the proposition
  might change depending on the variables you pass in, which seems like
  something you'd want to disallow.

  I think you could set P to be `E -> Prop` with a pre-determined countable set
  of variables, and had an equivalent induction principle like this, using
  cofinite induction:

         forall x, P x
         forall n, P n
         forall s, P s
         forall x x' e1 e2, exists X: x' notin X ->
              P e1 -> P ( [ x' / x ] e2 ) -> P (let x be e1 in e2)
         forall e1 e2, P e1 -> P e2 -> P (e1 `op` e2)

   What are the advantages of the approach taken in PFPL?

+ Is structural induction modulo fresh renaming a generalization of
  cofinite quantification?

+ In defining the judgements of local closure, we used co-finite
quantification to acquire a stronger induction hypothesis in the lc_let
case. From a mechanical point of view, I understand the purpose of L, but I
was wondering if there's some intuitive interpretation we can apply to it?

For example, later on when Ch4.v talked about the induction principles of
ABTs, the set of free variables X indexing each ABT acts like L, and it has an
interpretation being the set of free variables that can appear in this ABT,
but I'm having some trouble wrapping my head around whether I can think about
L in the same way. I believe that in practice, L can be chosen to be any set
larger than (fv e2) (set of free variables in the let body), but is that an
appropriate way to think about it?

+ Was the motivation for cofinite quantification due to technical difficulties
encountered when formalizing metatheory, like the one in the tutorial? It
isn't clearly a priori that the initial definition would run into trouble
later on. It is worrisome that one may go about doing quite a lot of work
before finding out that something at the root should be better revised. I
suppose one could patch this by gluing on statements expressing some form of
equivalence, but that's hardly ideal.

For example, one possibility of defining local closure is to count the number
of binders from the variable to the root. How do we choose the "right"
strategy for such?

+ Apart from proving alpha-equivalence, what are other fields that
structural induction modulo fresh renaming can be used?

+ In the notes it's stated that the renaming operation [x <~> y] replaces
  every instance of x with y and every instance of y with x. Typically we seem
  to use this operation when y is free, is there a situation we'd need it when
  x and y both appear in the expression?

# Other questions

+  I have been reading Chapter 3 and 4 of PFPL and found the same symbol
   ⊢ in both chapters, representing derivability judgement of statements
   and typing in language E. Is it an instance of Curry–Howard
   correspondence between derivations and typings?

+ I've often wanted to do something like "pick fresh x in L", "let x be ...",
etc. But I haven't been sufficiently well-versed with the tactic language and
CIC to go about doing it. Would you be going through this in class?

+ In the draft version of the book, in chapter 4.3, page 38, the author
  commented on the weakening lemma saying that from a programming point of
  view, the weakening property allows programmers to add new variables and it
  won't break the existing program. However, in the footnote, there were also
  comments about systems that don't obey the weakening property yet they are
  still useful. I am wondering what interesting capabilities that these
  substructural systems might have over systems that do obey the weakening
  property? It seems to be such a fundamental law for any programming language
  and I can't really think of cases where not having this property can be
  useful.

On top of that, it seems the weakening property, substitution property and the
decomposition property together tells us that the simple language described in
chapter 4 is type-safe. Are these the required properties for a language to
have to be called type-safe? Can a language violate some of them yet still be
called type-safe?

+ Homework exercise:

     Lemma subst_open : forall (x : atom) u e1 e2,
          lc u ->
          open ([x ~> u] e1) e2 = [x ~> u] (open e1 e2).
     Proof.
     (* EXERCISE *) Admitted.

Say e2 = exp_fvar x, e1 = exp_bvar 0.
So ([x ~> u]) e1 = e1 = exp_bvar 0. open ([x ~> u] e1) e2 = e2 = exp_fvar x.
Again, (open e1 e2) = exp_fvar x. So [x ~> u] (open e1 e2) = u.
Now exp_fvar x is not necessarily equal to u. How do we resolve this?


+ You state that "If we view typing contexts as ordered lists of typing
  assumptions, then the type system shown in section 4.2 of PFPL does NOT
  satisfy the weakening property" after the statement of weakening. I'm not
  quite sure what is meant by this. Does it mean that we can only use the types
  in the context in a certain order? What kind of type system does this
  describe?





# Nov 3, 3016

# Untyped Lambda Calculus
## Kenny Foner

# Questions

## Statics and Dynamics

- What do the statics tell us exactly? Because our contexts just contain
  variables which don't map to types, and it seems to me that a well-defined
  expression is just one that is syntatically correct via the grammar, so I
  don't understand what new information the statics provide.

  [SCW:  The statics only talk about scoping.  If we know that
     |- u ok  then we know that u does not have any free variables. ]

- It seemed that provided Gamma that contains all variants x_i, we can derive
  that all expressions are well-typed. This reminds me of the previous
  chapters, where a Delta that contains all type variants t_i can derive all
  type expressions. Is there any relation between these two judgments?

- Why are the dynamics given equationally rather than via a transition system?
  What exactly is the difference?

- Why does Bob Harper give the dynamics for Lambda calculus equationally? How
  would a transition system look, I naively think it would work with just a
  single transition rule for function application via substitution.

  [SCW: I'm not sure why he does this. Perhaps to set up for the next chapter
  where he wants to talk about definability and needs equality to reason about
  these definitions.

  Note that Bob is also gives the CBN version of the beta-equivalence rule, so this
  definition corresponds to a CBN transition system. We'd need the beta rule
  as well as one other rule to reduce th function part of an application if it
  was not a value.]

- Could we define the dynamics using transition system? If we can, the value
  of the language will be abstraction only?

  [SCW: Yes, the values of this language are only lambda abstractions.]



## Encodings

- Could we also see other, more unusual fix point combinators? Out of
  curiosity, are there other, possibly more convoluted, ways of encoding
  recursion?

  [SCW: There are many, many fixed point combinators. Here is one 
  from wikipedia, called the Turing combinator:
	 Theta = (\x. \y. (y (x x y))) (\x. \y. (y (x x y)))

  We can derive it via the following exercise:
  Find a fixed point combinator Q such that Q = A A, where A is of the
  form: λa.λb.e.  How to solve it? We must have:
        Q F = A A F = e [a → A] [b → F ] = F (A A F ).
  We can guess that A should be defined as: A = λa.λb. b (a a b) and
  check that indeed it works.

  This generalizes:
  Find a fixed point combinator Q such that Q = B B B, where B is of the
  form  λa.λb.λc.e.  How to solve it? We must have:
        Q F = B B B F = e [a → B] [b → B] [c → F] = F (B B B F).
  We can guess that B should be defined as: B = λa.λb.λc. c (a a b c) and
  check that indeed it works. But this is not the only B that works.]

- My question for tomorrow is on the Y combinator. If I remember correctly,
  this was the form that caused divergence under eager semantics in the last
  lecture, but it seems fine over here in the untyped lambda-calculus
  setting. How does the eager semantics in the transition system before fit
  with the equational dynamics given here? Can I say that there's no problem
  here because all values are functions?

  [SCW: The equational dynamics given in the chapter is for CBN, so the
  usual Y combinator works.  If we had restricted rule 21.2f to CBV, then
  we would need to use the Y combinator that I presented last time.]

- This is not really something I'm puzzled about, but I really enjoy seeing
  how Church encoding is just pattern matching presented differently (and
  relying on the unique type). Could we see more advanced examples of this? If
  time allows, could we even make the transformation (from some cool language)
  precise? Obviously, since this is just for fun, this is low priority

  [SCW: You may be interested in Scott encodings, which provide *only* pattern
  matching and not recursion. They are fun in the lambda-calculus because we
  already have recursion from the Y combinator.]

- Section 21.2 says, "This assertion is true for all _known_ means of defining
  computable functions on the natural numbers. The force of Church's law is
  that it postulates that all future notions of computation will be equivalent
  in expressive power to the lambda calculus." What are the different known
  means of defining computable functions? What does 'notion of computation'
  mean?

  [SCW: Here Bob is saying that the lambda-calculus is a universal model of
  computation, like a Turing machine. In fact, he prefers it to Turing machines
  when thinking about computability.]

## Undecidability (Scott's theorem)

- If possible, I'd appreciate going over the proof of Scott's theorem
  together, to heighten my understanding of it

- Even after having read the chapter to prepare for presenting it (prior to
  the switch with Kenny), I'm still lacking some intuition behind Scott's
  theorem. I can follow most of the details of the proof, but it still feels a
  bit weird somehow. If you are planning on covering Scott's theorem I would
  appreciate any intuition offered :)

- What can we get from the result that the definitional equality for the
  untyped lambda calculus is undecidable?

  [SCW: if we didn't have Turing's undecidability of the halting problem
  already, we would have another source of undecidability.]

- In untyped lambda calculus, as someone outside of the language reasoning
  about terms, we can tell when two terms are alpha-equivalent. Even though
  Scott's theorem tells us that general definitional equality is undecidable
  for lambda calculus, is it possible to somehow encode alpha-equivalence
  testing inside the language for specific classes of terms (the church
  numerals maybe)?

## Untyped means uni-typed

- It is claimed that every expression in the untyped lambda calculus has only
  one type i.e. the type D which satisfies the equation t = t -> t. What is
  the explicit isomorphism that confirms this assertion? By explicit I mean
  something like nat being isomorphic to unit + nat via the isomorphism f(0) =
  inl(()) and f(s(n)) = inr(n) and its inverse g(inl()) = 0 and g(inr(n)) =
  s(n). I tried defining F(f) = \lambda x.f and G(g) = g g so that G(F (f)) =
  G(\lambda x. f) = (\lambda x. f) (\lambda x. f) = f which is correct but
  F(G(g)) = F (g g) which is impossible to reason about so is almost certainly
  wrong.

- We have "D is a type that is isomorphic to the space of functions on D
  itself, something that is impossible in conventional set theory, but is
  feasible in the computationally based setting of lamda calculus".
  I see why this doesn't work in set theory. I don't actually see why it works
  in Lambda calculus and why?

  [SCW: The reason that we can do it here is that the function space D -> D
  is not the set of all functions from D to D (which is too big to be isomorphic
  to D, but the set of all *computable* functions from D to D. We can restrict
  our attention to computable functions, because we only care about the ones
  that we can express in the lambda-calculus. ]

- I understand why the untyped lambda-calculus is really uni-typed
  lambda-calculus, but what’s the benefit in thinking this way?

  [SCW: We'll see that next time --- basically, if we observe that there is one
  type, it tells us how to add more types to it.]

- In Section 21.4 the book says “It is not the absence of types that gives it
  its power, but rather it has only one type.” Why would “having only one
  type” gives a language power?

  [SCW: The power comes from the one type itself, which is expressive enough
  to encode recursion.]

- What is the relation between untyped(uni-typed) lambda calculus and simple
  typed lambda calculus?

  [SCW: Not sure what you are asking. The untyped lambda calculus is a partial
  language, but the simply-typed lambda calculus is total. So there are programs
  (like Y) that we can write in the untyped language that we cannot write in
  STLC.]



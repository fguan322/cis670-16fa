# Nov 8, 2016

## Dynamic typing
## Speaker: Omar

# Questions

- Is there any special reason why Bob decides to phrase the runtime
  typechecking as is_num and is_fun judgments rather than phrasing it closer
  to a typing judgment?

[SCW: I think it bothers Bob that this is called runtime *type* checking in
the first place. Operationally, we're just checking tags on data. Furthermore,
the tag checking is shallow---it doesn't look deeply at the structure of its
argument like a typing judgment might.]

- In DPCF, we only have num, fun, nil and cons and we already need to define
  more rules for run-time type checking. If there are more types, will the
  increasing number of rules be a problem?

[SCW: Dynamic languages either stick with a relatively small set of types
(i.e.  numbers, characters, strings/symbols, functions, nil, pairs, and thats
it) or include a mechanism for adding new user defined types defined in terms
of this small set, such as Haskell's "newtype" mechanism.]

- In Section 22.3, I feel that the disadvantages pointed out outweigh the
  advantages pointed out regarding dynamic typing. In such a case, where and
  how does dymanic typing stand wrt static typing? And also, is it not a good
  thing to catch mistakes as early as possible?

[SCW: Bob has made up his mind here. You are not going to get a thorough
discussion about the *advantages* of dynamic typing from him. Or from me, for
that matter.]

- I am not clear of the critique of dynamic typing given by Bob. Does he mean
  that the dynamic typing needs redundant check which should be avoided when
  possible? However, there are some dynamic typing languages which have been
  widely used. Moreover, static language have some features for dynamic
  typing, right?

[SCW: These are all good points. Static languages are more expressive because
they can eliminate runtime checks (which are required for safe execution of
dynamic languages). Furthermore, dynamic languages can be embedded in static
ones: see Haskell's Data.Dynamic library, for example. However, there is a
"paradox" that some dynamic languages, like Javascript and Python, are
currently very popular. We should discuss this paradox in class.]

- What's the reason for the check that there are no free variables in our
  expression? Since we're working in a dynamic language that allows error
  states, wouldn't it be fine to check this at runtime as well? This seems
  kind of inconsistent to me.

  [SCW: There is such a thing as dynamic scoping, where variables are resolved
  at runtime. This often leads to confusing semantics for higher-order
  functions.  For example, what result should this code return:

      let x = 2 in
	   let f = \y -> x in
	   let x = 3 in
   	f ()

  With static scoping, we get 2, and with dynamic scoping we get 3. Experience
  has shown that dynamic scoping is hard for users to reason about --- so
  elisp is the only language that I know of that still support it. (Well,
  Haskell can encode a form of it via type classes, but in that case it is
  controlled by the type system.) All other dynamic languages stick with
  static scoping.
  
  Perhaps it would also be possible to report errors from static scoping at
  runtime, but there would be no benefit for it. All you would be doing is
  delaying errors.]


- To what degree are the performance problems pointed out in 22.3 solvable by
  optimization/static analysis?

  [SCW: Many dynamic languages do use static analysis for optimization, in
  order to remove dynamic checks. The difficulty lies in making these
  optimizations interprocedural---in Haskell/ML function types allow this
  information to flow naturally across function calls. But this can be a
  challenge to static analysis.]

- Not that I *like* dynamic typing, but it feels like the criticism of dynamic
  typing in the book feels rather weak. I mean, yes, DPCF can't express the
  invariants necessary to avoid some of the runtime overheads, but it is not
  hard to imagine an extended version of the language that does. There is even
  work on adding type-system-like behavior as layer above dynamic languages
  (e.g. http://learnyousomeerlang.com/dialyzer). I guess, my question is why
  we focus so much on why *DPCF* is bad instead of why *dynamic typing in
  general* is bad (programmer sanity?).

  [SCW: Good question. Programmer sanity is harder to gather evidence for than
  performance. On one hand you have to run user studies, on the other, you
  only need to time your compiler.]

- The book says that the safety theorem holds for DPCF, and “every piece of
  abstract in DPCF is well-formed”. I don’t understand the reason of defining
  safety theorem in this way. Wouldn’t that make the safety theorem kind of
  weak? Because now it seems any language would be safe as long as all the
  runtime errors are well specified in the dynamics.

  [SCW: That is exactly the point. The safety theorem is weak! What it means
  is that if someone comes to you and tells you that their language is safe,
  you have to look closely at their semantics to be able to gauge the import
  of that statement. (Regardless though, safe is always better than
  unsafe. Safety means that you have correctly identified all possible runtime
  errors.)]